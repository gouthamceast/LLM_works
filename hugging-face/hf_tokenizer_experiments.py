# -*- coding: utf-8 -*-
"""HF_tokenizer_experiments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16BSEAz1KdFdEMxEnsfvn1B_MaC-qojRu
"""

from google.colab import userdata

from huggingface_hub import login
from transformers import AutoTokenizer

hf_token = userdata.get('HF_TOKEN')
login(hf_token, add_to_git_credential=True)

#Tokenizer llama

#first accept TC from huggingface to use llama model

tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B')

text = "started using llama3 model, kudos to hugginface"
tokens = tokenizer.encode(text)

tokens

tokenizer.decode(tokens)

chat_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct')

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello, who are you?"},
    {"role": "assistant", "content": "I am a language model"},
]

prompt = chat_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

print(prompt)

#other models

PHI3_MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"
QWEN2_MODEL_NAME = "Qwen/Qwen2-7B-Instruct"
STARCODER2_MODEL_NAME = "bigcode/starcoder2-3b"

phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)

text = "I am checking another tokenizer and learning about them from LLM course"
print(phi3_tokenizer.encode(text))

qwen = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)
print(qwen.encode(text))

star = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME)
print(star.encode(text))

